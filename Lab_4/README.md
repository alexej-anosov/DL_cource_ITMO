# Лабораторная работа №4

## Цель работы
Участие в соревновании https://github.com/dialogue-evaluation/RuREBus по NER.

## Данные

Обучающая и тестовая выборки были представлены в [репозитории](https://github.com/dialogue-evaluation/RuREBus). 

## Решение

Согласно варианту задания, была выбрана модель BiLSTM-CRF.

В качестве входных данных использовались эмбеддинги, полученные с помощью предобученной модели [ruBert](https://huggingface.co/DeepPavlov/rubert-base-cased). 


## Результаты

Были проведены эксперименты с различным размером скрытого состояния: 64, 128, 256. Результаты представлены ниже.


![Validation token-level F1-micro](/images/validation_token_level.png "Validation token_level F1-micro")

В качестве возможных улучшений можно предложить:
1) Дообучать Bert на корпусе схожих с доменом текстов, что его эмбеддинги были максимально релевантные
2) Разметить больше тренировочных данных с помощью LLM, дообученной на имеющейся разметке

