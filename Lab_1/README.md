# Лабораторная работа №1

## Цель работы
Разработка методов глубокого обучения в задачах классификации изображений.

## Moдель
В качестве модели, согласно варианту, была выбрана модель [MobileNetV2](https://arxiv.org/abs/1801.04381)

Ее ключевыми особенностями являются:

1) Depthwise Separable Convolutions

Данный блок является важным компонентом многих эффективных нейронных сетей, так как значительно более эффективный по параметрам и операциям по сравнению с классической сверткой.

2) Inverted residuals.

В классическом residual block, skip connection делается между слоями с большим числом каналов, а в середине находятся слои с меньшим числом каналов. Авторы MobileNetV2 сделали наоборот: skip connection делается между слоями с меньшим числом слоев. Авторы говорят, что такая схема в их реализации даже более эффективная по памяти, и показывает слегка лучшие результаты.

3) Linear Bottlenecks

Обычно мы используем функции активации, например ReLU, чтобы добавить ниленейность в нашу модель. Однако, при применении функции активации теряется часть информации. Обычно, это компенсируется увеличением числа каналов, однако в inverted residuals block авторы наоборот уменьшают число каналов. В связи с этим авторы решили убирать последнюю функцию активации блока.

4) Функция активации ReLU6

Классическая функция активации ReLU вычисляется по формуле max(0, x). Используемая авторами ReLU6 вычисляется по формуле min(max(0, x), 6). Функция ограничивает не только минимум с помощью 0, но и максимум с помощью 6. Это позволяет увеличить устойчивость модели при low-precision вычислениях.


## Оптимизаторы
Согласно варианту были рассмотрены два оптимизатора: Adam и Adamax.


## Данные

Был взят [Cars Dataset](https://deepvisualmarketing.github.io/). Рассматривалась задача классификации цвета автомобиля. Было отобрано 11 цветов, взято по 10000 изображений в обучающую выборку и по 1000 в тестовую для каждого цвета.

## Результаты
По резльтатам обучения можно сказать, что для данной задачи с данными гиперпараметрами обучения, оптимизатор Adamax показал более стабильное обучени и немного лучший результат, по сравнению с оптимизатором Adam. Однако, Adamax оказался чуть медленнее, 1:54:34  против 1:38:34.

![Validation MobileNetV2 loss](/Lab_1/images/validation_loss.png "Validation MobileNetV2 loss")

![Validation MobileNetV2 F1-macro](/Lab_1/images/validation_f1macro.png "Validation MobileNetV2 F1-macro")

UPD: Также была претестирована MobileNetV1 с оптимизатором Adam. Точность MobileNetV1 ниже, но скорость обучения выше - 1:11:21.

![Validation MobileNetV1 loss](/Lab_1/images/mobilenetv1_validation.png "Validation MobileNetV1 loss")

![Validation MobileNetV1 F1-macro](/Lab_1/images/mobilenetv1_f1.png "Validation MobileNetV1 F1-macro")



