{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3322690,"sourceType":"datasetVersion","datasetId":2008576}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom IPython.display import clear_output\nimport gc\nimport sys\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-09T08:25:51.376125Z","iopub.execute_input":"2024-03-09T08:25:51.376521Z","iopub.status.idle":"2024-03-09T08:25:51.772334Z","shell.execute_reply.started":"2024-03-09T08:25:51.376477Z","shell.execute_reply":"2024-03-09T08:25:51.771535Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/youtube-videos-dataset/youtube.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2024-03-09T08:25:54.036189Z","iopub.execute_input":"2024-03-09T08:25:54.037238Z","iopub.status.idle":"2024-03-09T08:25:54.136734Z","shell.execute_reply.started":"2024-03-09T08:25:54.037202Z","shell.execute_reply":"2024-03-09T08:25:54.135654Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"             link                                              title  \\\n0         JLZlCZ0  Ep 1| Travelling through North East India | Of...   \n1     i9E_Blai8vk      Welcome to Bali | Travel Vlog | Priscilla Lee   \n2      r284c-q8oY  My Solo Trip to ALASKA | Cruising From Vancouv...   \n3      Qmi-Xwq-ME   Traveling to the Happiest Country in the World!!   \n4     _lcOX55Ef70  Solo in Paro Bhutan | Tiger's Nest visit | Bhu...   \n...           ...                                                ...   \n3594       #NAME?  21st Century Challenges: Crash Course European...   \n3595  d-2Trw8bCa0  EU DataViz webinar - Barnaby Skinner - How to ...   \n3596    RCKWarkUL  Stone Age Scandinavia: First People In the Nor...   \n3597   MF6F3BxJIY  AP European History - Interwar Period: Paris P...   \n3598   lByKodp_UK  World War 2 Allied Conferences: AP European Hi...   \n\n                                            description category  \n0     Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nT...   travel  \n1     Priscilla Lee\\n45.6K subscribers\\nSUBSCRIBE\\n*...   travel  \n2     Allison Anderson\\n588K subscribers\\nSUBSCRIBE\\...   travel  \n3     Yes Theory\\n6.65M subscribers\\nSUBSCRIBE\\n*BLA...   travel  \n4     Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nH...   travel  \n...                                                 ...      ...  \n3594  CrashCourse\\n12.4M subscribers\\nSUBSCRIBE\\nThe...  history  \n3595  Publications Office of the European Union\\n3.2...  history  \n3596  History Time\\n619K subscribers\\nSUBSCRIBE\\n- W...  history  \n3597  Mr. Raymond's Civics and Social Studies Academ...  history  \n3598  Paul Sargent\\n25.3K subscribers\\nSUBSCRIBE\\nIn...  history  \n\n[3599 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>link</th>\n      <th>title</th>\n      <th>description</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>JLZlCZ0</td>\n      <td>Ep 1| Travelling through North East India | Of...</td>\n      <td>Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nT...</td>\n      <td>travel</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i9E_Blai8vk</td>\n      <td>Welcome to Bali | Travel Vlog | Priscilla Lee</td>\n      <td>Priscilla Lee\\n45.6K subscribers\\nSUBSCRIBE\\n*...</td>\n      <td>travel</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>r284c-q8oY</td>\n      <td>My Solo Trip to ALASKA | Cruising From Vancouv...</td>\n      <td>Allison Anderson\\n588K subscribers\\nSUBSCRIBE\\...</td>\n      <td>travel</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Qmi-Xwq-ME</td>\n      <td>Traveling to the Happiest Country in the World!!</td>\n      <td>Yes Theory\\n6.65M subscribers\\nSUBSCRIBE\\n*BLA...</td>\n      <td>travel</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>_lcOX55Ef70</td>\n      <td>Solo in Paro Bhutan | Tiger's Nest visit | Bhu...</td>\n      <td>Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nH...</td>\n      <td>travel</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3594</th>\n      <td>#NAME?</td>\n      <td>21st Century Challenges: Crash Course European...</td>\n      <td>CrashCourse\\n12.4M subscribers\\nSUBSCRIBE\\nThe...</td>\n      <td>history</td>\n    </tr>\n    <tr>\n      <th>3595</th>\n      <td>d-2Trw8bCa0</td>\n      <td>EU DataViz webinar - Barnaby Skinner - How to ...</td>\n      <td>Publications Office of the European Union\\n3.2...</td>\n      <td>history</td>\n    </tr>\n    <tr>\n      <th>3596</th>\n      <td>RCKWarkUL</td>\n      <td>Stone Age Scandinavia: First People In the Nor...</td>\n      <td>History Time\\n619K subscribers\\nSUBSCRIBE\\n- W...</td>\n      <td>history</td>\n    </tr>\n    <tr>\n      <th>3597</th>\n      <td>MF6F3BxJIY</td>\n      <td>AP European History - Interwar Period: Paris P...</td>\n      <td>Mr. Raymond's Civics and Social Studies Academ...</td>\n      <td>history</td>\n    </tr>\n    <tr>\n      <th>3598</th>\n      <td>lByKodp_UK</td>\n      <td>World War 2 Allied Conferences: AP European Hi...</td>\n      <td>Paul Sargent\\n25.3K subscribers\\nSUBSCRIBE\\nIn...</td>\n      <td>history</td>\n    </tr>\n  </tbody>\n</table>\n<p>3599 rows Ã— 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.value_counts('category')","metadata":{"execution":{"iopub.status.busy":"2024-03-09T08:25:54.711322Z","iopub.execute_input":"2024-03-09T08:25:54.712393Z","iopub.status.idle":"2024-03-09T08:25:54.731979Z","shell.execute_reply.started":"2024-03-09T08:25:54.712358Z","shell.execute_reply":"2024-03-09T08:25:54.730947Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"category\ntravel       1156\nart_music     947\nfood          903\nhistory       593\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"labels = set(df['category'].values)\nn_labels = len(labels)\n\nlabel_index_map = {l: i for i, l in enumerate(labels)}\nindex_label_map = {i: l for l, i in label_index_map.items()}","metadata":{"execution":{"iopub.status.busy":"2024-03-09T08:25:55.022359Z","iopub.execute_input":"2024-03-09T08:25:55.022693Z","iopub.status.idle":"2024-03-09T08:25:55.028410Z","shell.execute_reply.started":"2024-03-09T08:25:55.022669Z","shell.execute_reply":"2024-03-09T08:25:55.027374Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import f1_score\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-03-09T08:25:55.507449Z","iopub.execute_input":"2024-03-09T08:25:55.507842Z","iopub.status.idle":"2024-03-09T08:26:01.989198Z","shell.execute_reply.started":"2024-03-09T08:25:55.507813Z","shell.execute_reply":"2024-03-09T08:26:01.988180Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_length = 256\n_pretrained_model = 'google-bert/bert-base-uncased'\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'output_hidden_states':True})\nmodel = AutoModel.from_pretrained(_pretrained_model, config=config)\ntokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n\nclear_output()\n\nmodel = model.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-03-09T08:26:10.138801Z","iopub.execute_input":"2024-03-09T08:26:10.139742Z","iopub.status.idle":"2024-03-09T08:26:11.011737Z","shell.execute_reply.started":"2024-03-09T08:26:10.139708Z","shell.execute_reply":"2024-03-09T08:26:11.010871Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"hidden_states_cls_dict = {}\nfor i in range(1, 13):\n    hidden_states_cls_dict[f'layer_{i}'] = []","metadata":{"execution":{"iopub.status.busy":"2024-03-09T08:26:12.832192Z","iopub.execute_input":"2024-03-09T08:26:12.832839Z","iopub.status.idle":"2024-03-09T08:26:12.837650Z","shell.execute_reply.started":"2024-03-09T08:26:12.832809Z","shell.execute_reply":"2024-03-09T08:26:12.836535Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"for i in range((len(df)//256)+1):\n    texts = df['title'][256*i:256*(i+1)].tolist()\n    features = tokenizer.batch_encode_plus(\n        texts,\n        add_special_tokens=True,\n        padding='max_length',\n        max_length=max_seq_length,\n        truncation=True,\n        return_tensors='pt',\n        return_attention_mask=True\n    )\n    features.to('cuda')\n    with torch.no_grad():\n        outputs = model(features['input_ids'], features['attention_mask'])\n        all_hidden_states = torch.stack(outputs[2])\n        for i in range(1, 13):\n            layer_cls_list = []\n            for ii in all_hidden_states[i]:\n                layer_cls_list.append(ii[0].tolist())\n            hidden_states_cls_dict[f'layer_{i}'].extend(layer_cls_list)  \n        \n        \n        del outputs, all_hidden_states, features, layer_cls_list, texts\n        gc.collect()\n        torch.cuda.empty_cache()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-09T08:26:13.290132Z","iopub.execute_input":"2024-03-09T08:26:13.290871Z","iopub.status.idle":"2024-03-09T08:26:53.117664Z","shell.execute_reply.started":"2024-03-09T08:26:13.290838Z","shell.execute_reply":"2024-03-09T08:26:53.116760Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"for i in range(1, 13):\n    df[f'layer_{i}'] = hidden_states_cls_dict[f'layer_{i}']","metadata":{"execution":{"iopub.status.busy":"2024-03-09T08:26:53.119386Z","iopub.execute_input":"2024-03-09T08:26:53.119829Z","iopub.status.idle":"2024-03-09T08:26:53.138475Z","shell.execute_reply.started":"2024-03-09T08:26:53.119788Z","shell.execute_reply":"2024-03-09T08:26:53.137553Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T08:26:53.139950Z","iopub.execute_input":"2024-03-09T08:26:53.140305Z","iopub.status.idle":"2024-03-09T08:26:53.177085Z","shell.execute_reply.started":"2024-03-09T08:26:53.140272Z","shell.execute_reply":"2024-03-09T08:26:53.175988Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"      link                                              title  \\\n0  JLZlCZ0  Ep 1| Travelling through North East India | Of...   \n\n                                         description category  \\\n0  Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nT...   travel   \n\n                                             layer_1  \\\n0  [0.20696742832660675, 0.009053017012774944, -0...   \n\n                                             layer_2  \\\n0  [0.08044908940792084, -0.200675368309021, -0.1...   \n\n                                             layer_3  \\\n0  [0.06040492653846741, -0.22809070348739624, -0...   \n\n                                             layer_4  \\\n0  [0.32965075969696045, -0.6098545789718628, -0....   \n\n                                             layer_5  \\\n0  [0.022259468212723732, -0.7881401181221008, -0...   \n\n                                             layer_6  \\\n0  [0.2084280103445053, -1.0606553554534912, -0.5...   \n\n                                             layer_7  \\\n0  [0.17031367123126984, -1.2389631271362305, -0....   \n\n                                             layer_8  \\\n0  [-0.1554407924413681, -1.0762848854064941, -0....   \n\n                                             layer_9  \\\n0  [-0.24458223581314087, -0.6536688208580017, -0...   \n\n                                            layer_10  \\\n0  [-0.671563446521759, -0.9311846494674683, 0.03...   \n\n                                            layer_11  \\\n0  [-0.6237320899963379, -0.3326026499271393, -0....   \n\n                                            layer_12  \n0  [-0.5532158017158508, 0.03186248615384102, 0.0...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>link</th>\n      <th>title</th>\n      <th>description</th>\n      <th>category</th>\n      <th>layer_1</th>\n      <th>layer_2</th>\n      <th>layer_3</th>\n      <th>layer_4</th>\n      <th>layer_5</th>\n      <th>layer_6</th>\n      <th>layer_7</th>\n      <th>layer_8</th>\n      <th>layer_9</th>\n      <th>layer_10</th>\n      <th>layer_11</th>\n      <th>layer_12</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>JLZlCZ0</td>\n      <td>Ep 1| Travelling through North East India | Of...</td>\n      <td>Tanya Khanijow\\n671K subscribers\\nSUBSCRIBE\\nT...</td>\n      <td>travel</td>\n      <td>[0.20696742832660675, 0.009053017012774944, -0...</td>\n      <td>[0.08044908940792084, -0.200675368309021, -0.1...</td>\n      <td>[0.06040492653846741, -0.22809070348739624, -0...</td>\n      <td>[0.32965075969696045, -0.6098545789718628, -0....</td>\n      <td>[0.022259468212723732, -0.7881401181221008, -0...</td>\n      <td>[0.2084280103445053, -1.0606553554534912, -0.5...</td>\n      <td>[0.17031367123126984, -1.2389631271362305, -0....</td>\n      <td>[-0.1554407924413681, -1.0762848854064941, -0....</td>\n      <td>[-0.24458223581314087, -0.6536688208580017, -0...</td>\n      <td>[-0.671563446521759, -0.9311846494674683, 0.03...</td>\n      <td>[-0.6237320899963379, -0.3326026499271393, -0....</td>\n      <td>[-0.5532158017158508, 0.03186248615384102, 0.0...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['category'], random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T08:26:53.179723Z","iopub.execute_input":"2024-03-09T08:26:53.180344Z","iopub.status.idle":"2024-03-09T08:26:53.203039Z","shell.execute_reply.started":"2024-03-09T08:26:53.180307Z","shell.execute_reply":"2024-03-09T08:26:53.202162Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def train(model, criterion, optimizer, train_loader):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n    return running_loss / len(train_loader.dataset)\n\ndef validate(model, criterion, val_loader):\n    model.eval()\n    running_loss = 0.0\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(predicted.cpu().numpy())\n    return running_loss / len(val_loader.dataset), f1_score(y_true, y_pred, average='macro')","metadata":{"execution":{"iopub.status.busy":"2024-03-09T08:29:26.483177Z","iopub.execute_input":"2024-03-09T08:29:26.483582Z","iopub.status.idle":"2024-03-09T08:29:26.493732Z","shell.execute_reply.started":"2024-03-09T08:29:26.483551Z","shell.execute_reply":"2024-03-09T08:29:26.492699Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Only 12th layer","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        batch = self.data.iloc[idx]\n        features = torch.tensor(batch['layer_12'], dtype=torch.float)\n        label = torch.tensor(label_index_map[batch['category']], dtype=torch.long)\n        return features, label\n    \nbatch_size = 128\ntrain_dataset = CustomDataset(train_df)\ntest_dataset = CustomDataset(test_df)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleModel, self).__init__()\n        self.conv = nn.Conv1d(1, 1, kernel_size=3, padding=0, stride=3)\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self.conv(x).squeeze()\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x\n    \n    \ninput_size = 256\nhidden_size = 512\noutput_size = n_labels\nlr = 0.001\nnum_epochs = 100\n\nmodel = SimpleModel(input_size, hidden_size, output_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)\nscheduler = StepLR(optimizer, step_size=1, gamma=0.99) \n\n\nfor epoch in range(num_epochs):\n    train_loss = train(model, criterion, optimizer, train_dataloader)\n    val_loss, val_f1 = validate(model, criterion, test_dataloader)\n    if (epoch+1)%10==0:\n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1 (macro): {val_f1:.4f}\")\n    scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2024-03-09T09:33:26.063434Z","iopub.execute_input":"2024-03-09T09:33:26.064380Z","iopub.status.idle":"2024-03-09T09:34:52.938345Z","shell.execute_reply.started":"2024-03-09T09:33:26.064346Z","shell.execute_reply":"2024-03-09T09:34:52.937162Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Epoch 10/100, Train Loss: 0.8048, Val Loss: 0.8088, Val F1 (macro): 0.9480\nEpoch 20/100, Train Loss: 0.7807, Val Loss: 0.7971, Val F1 (macro): 0.9496\nEpoch 30/100, Train Loss: 0.7708, Val Loss: 0.7928, Val F1 (macro): 0.9531\nEpoch 40/100, Train Loss: 0.7661, Val Loss: 0.7957, Val F1 (macro): 0.9487\nEpoch 50/100, Train Loss: 0.7612, Val Loss: 0.7899, Val F1 (macro): 0.9565\nEpoch 60/100, Train Loss: 0.7588, Val Loss: 0.7890, Val F1 (macro): 0.9565\nEpoch 70/100, Train Loss: 0.7573, Val Loss: 0.7894, Val F1 (macro): 0.9565\nEpoch 80/100, Train Loss: 0.7567, Val Loss: 0.7887, Val F1 (macro): 0.9565\nEpoch 90/100, Train Loss: 0.7564, Val Loss: 0.7886, Val F1 (macro): 0.9578\nEpoch 100/100, Train Loss: 0.7563, Val Loss: 0.7885, Val F1 (macro): 0.9565\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Only 11th layer","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        batch = self.data.iloc[idx]\n        features = torch.tensor(batch['layer_11'], dtype=torch.float)\n        label = torch.tensor(label_index_map[batch['category']], dtype=torch.long)\n        return features, label\n    \nbatch_size = 128\ntrain_dataset = CustomDataset(train_df)\ntest_dataset = CustomDataset(test_df)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleModel, self).__init__()\n        self.conv = nn.Conv1d(1, 1, kernel_size=3, padding=0, stride=3)\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self.conv(x).squeeze()\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x\n    \n    \ninput_size = 256\nhidden_size = 512\noutput_size = n_labels\nlr = 0.001\nnum_epochs = 100\n\nmodel = SimpleModel(input_size, hidden_size, output_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)\nscheduler = StepLR(optimizer, step_size=1, gamma=0.99) \n\n\nfor epoch in range(num_epochs):\n    train_loss = train(model, criterion, optimizer, train_dataloader)\n    val_loss, val_f1 = validate(model, criterion, test_dataloader)\n    if (epoch+1)%10==0:\n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1 (macro): {val_f1:.4f}\")\n    scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2024-03-09T09:37:56.112373Z","iopub.execute_input":"2024-03-09T09:37:56.113053Z","iopub.status.idle":"2024-03-09T09:39:19.628021Z","shell.execute_reply.started":"2024-03-09T09:37:56.113024Z","shell.execute_reply":"2024-03-09T09:39:19.626942Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Epoch 10/100, Train Loss: 0.8709, Val Loss: 0.8785, Val F1 (macro): 0.8683\nEpoch 20/100, Train Loss: 0.8253, Val Loss: 0.8530, Val F1 (macro): 0.8918\nEpoch 30/100, Train Loss: 0.8092, Val Loss: 0.8427, Val F1 (macro): 0.9079\nEpoch 40/100, Train Loss: 0.7962, Val Loss: 0.8410, Val F1 (macro): 0.9014\nEpoch 50/100, Train Loss: 0.7900, Val Loss: 0.8434, Val F1 (macro): 0.8998\nEpoch 60/100, Train Loss: 0.7835, Val Loss: 0.8411, Val F1 (macro): 0.9038\nEpoch 70/100, Train Loss: 0.7803, Val Loss: 0.8424, Val F1 (macro): 0.9017\nEpoch 80/100, Train Loss: 0.7782, Val Loss: 0.8402, Val F1 (macro): 0.9066\nEpoch 90/100, Train Loss: 0.7770, Val Loss: 0.8401, Val F1 (macro): 0.8994\nEpoch 100/100, Train Loss: 0.7755, Val Loss: 0.8371, Val F1 (macro): 0.9032\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Only 10th layer","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        batch = self.data.iloc[idx]\n        features = torch.tensor(batch['layer_10'], dtype=torch.float)\n        label = torch.tensor(label_index_map[batch['category']], dtype=torch.long)\n        return features, label\n    \nbatch_size = 128\ntrain_dataset = CustomDataset(train_df)\ntest_dataset = CustomDataset(test_df)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleModel, self).__init__()\n        self.conv = nn.Conv1d(1, 1, kernel_size=3, padding=0, stride=3)\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self.conv(x).squeeze()\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x\n    \n    \ninput_size = 256\nhidden_size = 512\noutput_size = n_labels\nlr = 0.001\nnum_epochs = 100\n\nmodel = SimpleModel(input_size, hidden_size, output_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)\nscheduler = StepLR(optimizer, step_size=1, gamma=0.99) \n\n\nfor epoch in range(num_epochs):\n    train_loss = train(model, criterion, optimizer, train_dataloader)\n    val_loss, val_f1 = validate(model, criterion, test_dataloader)\n    if (epoch+1)%10==0:\n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1 (macro): {val_f1:.4f}\")\n    scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2024-03-09T09:39:19.630257Z","iopub.execute_input":"2024-03-09T09:39:19.630764Z","iopub.status.idle":"2024-03-09T09:40:45.884669Z","shell.execute_reply.started":"2024-03-09T09:39:19.630726Z","shell.execute_reply":"2024-03-09T09:40:45.883759Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"Epoch 10/100, Train Loss: 0.8494, Val Loss: 0.8756, Val F1 (macro): 0.8724\nEpoch 20/100, Train Loss: 0.8133, Val Loss: 0.8478, Val F1 (macro): 0.9043\nEpoch 30/100, Train Loss: 0.8019, Val Loss: 0.8435, Val F1 (macro): 0.9026\nEpoch 40/100, Train Loss: 0.7877, Val Loss: 0.8350, Val F1 (macro): 0.9050\nEpoch 50/100, Train Loss: 0.7809, Val Loss: 0.8318, Val F1 (macro): 0.9136\nEpoch 60/100, Train Loss: 0.7774, Val Loss: 0.8332, Val F1 (macro): 0.9055\nEpoch 70/100, Train Loss: 0.7754, Val Loss: 0.8342, Val F1 (macro): 0.9093\nEpoch 80/100, Train Loss: 0.7743, Val Loss: 0.8345, Val F1 (macro): 0.9102\nEpoch 90/100, Train Loss: 0.7744, Val Loss: 0.8323, Val F1 (macro): 0.9081\nEpoch 100/100, Train Loss: 0.7724, Val Loss: 0.8342, Val F1 (macro): 0.9034\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Only 9th layer","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        batch = self.data.iloc[idx]\n        features = torch.tensor(batch['layer_9'], dtype=torch.float)\n        label = torch.tensor(label_index_map[batch['category']], dtype=torch.long)\n        return features, label\n    \nbatch_size = 128\ntrain_dataset = CustomDataset(train_df)\ntest_dataset = CustomDataset(test_df)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleModel, self).__init__()\n        self.conv = nn.Conv1d(1, 1, kernel_size=3, padding=0, stride=3)\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self.conv(x).squeeze()\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x\n    \n    \ninput_size = 256\nhidden_size = 512\noutput_size = n_labels\nlr = 0.001\nnum_epochs = 100\n\nmodel = SimpleModel(input_size, hidden_size, output_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)\nscheduler = StepLR(optimizer, step_size=1, gamma=0.99) \n\n\nfor epoch in range(num_epochs):\n    train_loss = train(model, criterion, optimizer, train_dataloader)\n    val_loss, val_f1 = validate(model, criterion, test_dataloader)\n    if (epoch+1)%10==0:\n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1 (macro): {val_f1:.4f}\")\n    scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2024-03-09T09:40:45.885941Z","iopub.execute_input":"2024-03-09T09:40:45.886260Z","iopub.status.idle":"2024-03-09T09:42:11.497801Z","shell.execute_reply.started":"2024-03-09T09:40:45.886233Z","shell.execute_reply":"2024-03-09T09:42:11.496808Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"Epoch 10/100, Train Loss: 0.8871, Val Loss: 0.8946, Val F1 (macro): 0.8707\nEpoch 20/100, Train Loss: 0.8337, Val Loss: 0.8641, Val F1 (macro): 0.8851\nEpoch 30/100, Train Loss: 0.8122, Val Loss: 0.8546, Val F1 (macro): 0.8912\nEpoch 40/100, Train Loss: 0.8002, Val Loss: 0.8512, Val F1 (macro): 0.8965\nEpoch 50/100, Train Loss: 0.7919, Val Loss: 0.8516, Val F1 (macro): 0.8913\nEpoch 60/100, Train Loss: 0.7887, Val Loss: 0.8465, Val F1 (macro): 0.9003\nEpoch 70/100, Train Loss: 0.7843, Val Loss: 0.8482, Val F1 (macro): 0.8931\nEpoch 80/100, Train Loss: 0.7812, Val Loss: 0.8455, Val F1 (macro): 0.9011\nEpoch 90/100, Train Loss: 0.7787, Val Loss: 0.8447, Val F1 (macro): 0.8980\nEpoch 100/100, Train Loss: 0.7771, Val Loss: 0.8442, Val F1 (macro): 0.9023\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 11th and 12th layers with aggregation using point-wise conv","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        batch = self.data.iloc[idx]\n        features11 = torch.tensor(batch['layer_11'], dtype=torch.float)\n        features12 = torch.tensor(batch['layer_12'], dtype=torch.float)\n        features = torch.stack([features11, features12])\n        label = torch.tensor(label_index_map[batch['category']], dtype=torch.long)\n        return features, label\n    \nbatch_size = 128\ntrain_dataset = CustomDataset(train_df)\ntest_dataset = CustomDataset(test_df)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleModel, self).__init__()\n        self.conv_pw = nn.Conv1d(2, 1, kernel_size=1, padding=0, stride=1)\n        self.conv = nn.Conv1d(1, 1, kernel_size=3, padding=0, stride=3)\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.conv_pw(x)\n        x = self.conv(x).squeeze()\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x\n    \n    \ninput_size = 256\nhidden_size = 512\noutput_size = n_labels\nlr = 0.001\nnum_epochs = 100\n\nmodel = SimpleModel(input_size, hidden_size, output_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)\nscheduler = StepLR(optimizer, step_size=1, gamma=0.99) \n\n\nfor epoch in range(num_epochs):\n    train_loss = train(model, criterion, optimizer, train_dataloader)\n    val_loss, val_f1 = validate(model, criterion, test_dataloader)\n    if (epoch+1)%10==0:\n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1 (macro): {val_f1:.4f}\")\n    scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2024-03-09T09:43:15.193549Z","iopub.execute_input":"2024-03-09T09:43:15.194488Z","iopub.status.idle":"2024-03-09T09:45:36.772179Z","shell.execute_reply.started":"2024-03-09T09:43:15.194446Z","shell.execute_reply":"2024-03-09T09:45:36.771072Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Epoch 10/100, Train Loss: 0.9686, Val Loss: 0.9651, Val F1 (macro): 0.6370\nEpoch 20/100, Train Loss: 0.7949, Val Loss: 0.8070, Val F1 (macro): 0.9366\nEpoch 30/100, Train Loss: 0.7801, Val Loss: 0.8001, Val F1 (macro): 0.9430\nEpoch 40/100, Train Loss: 0.7714, Val Loss: 0.7961, Val F1 (macro): 0.9516\nEpoch 50/100, Train Loss: 0.7663, Val Loss: 0.7938, Val F1 (macro): 0.9532\nEpoch 60/100, Train Loss: 0.7620, Val Loss: 0.7939, Val F1 (macro): 0.9546\nEpoch 70/100, Train Loss: 0.7573, Val Loss: 0.7896, Val F1 (macro): 0.9554\nEpoch 80/100, Train Loss: 0.7565, Val Loss: 0.7888, Val F1 (macro): 0.9579\nEpoch 90/100, Train Loss: 0.7558, Val Loss: 0.7882, Val F1 (macro): 0.9563\nEpoch 100/100, Train Loss: 0.7551, Val Loss: 0.7886, Val F1 (macro): 0.9546\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Last 4 layers with aggregation using point-wise conv","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        batch = self.data.iloc[idx]\n        features9 = torch.tensor(batch['layer_9'], dtype=torch.float)\n        features10 = torch.tensor(batch['layer_10'], dtype=torch.float)\n        features11 = torch.tensor(batch['layer_11'], dtype=torch.float)\n        features12 = torch.tensor(batch['layer_12'], dtype=torch.float)\n        features = torch.stack([features9, features10, features11, features12])\n        label = torch.tensor(label_index_map[batch['category']], dtype=torch.long)\n        return features, label\n    \nbatch_size = 128\ntrain_dataset = CustomDataset(train_df)\ntest_dataset = CustomDataset(test_df)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleModel, self).__init__()\n        self.conv_pw = nn.Conv1d(4, 1, kernel_size=1, padding=0, stride=1)\n        self.conv = nn.Conv1d(1, 1, kernel_size=3, padding=0, stride=3)\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.conv_pw(x)\n        x = self.conv(x).squeeze()\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n        return x\n    \n    \ninput_size = 256\nhidden_size = 512\noutput_size = n_labels\nlr = 0.001\nnum_epochs = 100\n\nmodel = SimpleModel(input_size, hidden_size, output_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)\nscheduler = StepLR(optimizer, step_size=1, gamma=0.99) \n\n\nfor epoch in range(num_epochs):\n    train_loss = train(model, criterion, optimizer, train_dataloader)\n    val_loss, val_f1 = validate(model, criterion, test_dataloader)\n    if (epoch+1)%10==0:\n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val F1 (macro): {val_f1:.4f}\")\n    scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2024-03-09T09:50:14.313804Z","iopub.execute_input":"2024-03-09T09:50:14.314478Z","iopub.status.idle":"2024-03-09T09:53:58.030044Z","shell.execute_reply.started":"2024-03-09T09:50:14.314447Z","shell.execute_reply":"2024-03-09T09:53:58.028848Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Epoch 10/100, Train Loss: 0.8704, Val Loss: 0.8635, Val F1 (macro): 0.8886\nEpoch 20/100, Train Loss: 0.8106, Val Loss: 0.8400, Val F1 (macro): 0.9052\nEpoch 30/100, Train Loss: 0.7879, Val Loss: 0.8297, Val F1 (macro): 0.9093\nEpoch 40/100, Train Loss: 0.7778, Val Loss: 0.8267, Val F1 (macro): 0.9105\nEpoch 50/100, Train Loss: 0.7732, Val Loss: 0.8247, Val F1 (macro): 0.9137\nEpoch 60/100, Train Loss: 0.7715, Val Loss: 0.8235, Val F1 (macro): 0.9177\nEpoch 70/100, Train Loss: 0.7700, Val Loss: 0.8250, Val F1 (macro): 0.9131\nEpoch 80/100, Train Loss: 0.7679, Val Loss: 0.8245, Val F1 (macro): 0.9183\nEpoch 90/100, Train Loss: 0.7674, Val Loss: 0.8248, Val F1 (macro): 0.9164\nEpoch 100/100, Train Loss: 0.7658, Val Loss: 0.8260, Val F1 (macro): 0.9114\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}